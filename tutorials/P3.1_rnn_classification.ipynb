{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/vlamen/tue-deeplearning/blob/main/tutorials/P3.1_rnn_classification.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2LRRUCGNl_U5"
      },
      "source": [
        "There are some compatibility issues with the default colab packages. We will need to install new packages, which might take a few minutes."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bfsQbg7EprZx"
      },
      "outputs": [],
      "source": [
        "# !pip install torch==2.3.0 torchtext portalocker torchdata==0.8.0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IxOVma7mHME2"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torchtext\n",
        "from tqdm.notebook import tqdm\n",
        "%pylab inline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N_0fcsmrHME9"
      },
      "source": [
        "# P3.1 - Sequence classification: Predicting news article genres\n",
        "\n",
        "In this tutorial, we provide an example of text classification using the AG News dataset. The dataset contains 120.000 datapoints for training and 7600 datapoints for testing. Each datapoint is an input news article (given as a sequence of words $X:\\{x_{0:n}\\}$) and a label ($Y$) which encodes the article's genre with a class id (integer). More specifically, the class ids are numbered 1-4 where 1 represents World, 2 represents Sports, 3 represents Business and 4 represents Sci/Tech. Our goal is to train a model that predicts the genre of an input news article."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PcSBMbifHME9"
      },
      "source": [
        "### Loading dataset\n",
        "\n",
        "The torchtext library provides a few dataset iterators, which yield the raw text strings. For example, the ``AG_NEWS`` dataset iterators yield the raw data as a tuple of label $(Y)$ and text $(X)$. Here we load the train and test iterators and print a few examples. We will get some warnings because of old versions of packages."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aWgiI8pJHME-"
      },
      "outputs": [],
      "source": [
        "from torchtext.datasets import AG_NEWS\n",
        "import portalocker\n",
        "\n",
        "train_iter, test_iter = AG_NEWS(split=('train', 'test'))\n",
        "train_data = list(train_iter)\n",
        "test_data = list(test_iter)\n",
        "\n",
        "print(f\"Number of training news articles: {len(train_data)}\")\n",
        "print(f\"Number of testing news articles: {len(test_data)}\\n\\n\")\n",
        "\n",
        "classdict = {1:\"world\", 2:\"sports\", 3:\"business\", 4:'sci/tech'}\n",
        "\n",
        "for i in range(3):\n",
        "    y, x = test_data[i]\n",
        "    print(\"X: \" + x)\n",
        "    print(\"Y: \" + classdict[y] + '\\n')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tu3LMAJmHME-"
      },
      "source": [
        "# Preprocessing textual input data\n",
        "\n",
        "### Create vocabulary\n",
        "As we have seen in practical P1.2, word embeddings are useful for encoding words into dense vectors of real numbers.  The first step is to build a custom vocabulary from the raw training dataset. To this end, we tokenize each article, i.e. transform the full article in string format into a list of individual tokens that make up that string, e.g. \"good morning!\" becomes [\"good\", \"morning\", \"!\"]. Note that there is a subtle difference between tokens and words. For example, \"good\" and \"morning\" are both words and tokens, but \"!\" is a token, not a word.\n",
        "\n",
        "When all sentences are tokenized we proceed with counting the number of occurances of each token in each of the articles using `counter`. Finally, we create the vocabulary by using the frequencies of each token in the counter."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "intnYlNzHME-"
      },
      "outputs": [],
      "source": [
        "from torchtext.data.utils import get_tokenizer\n",
        "from collections import Counter\n",
        "from torchtext.vocab import vocab\n",
        "\n",
        "tokenizer = get_tokenizer('basic_english')\n",
        "\n",
        "counter = Counter()\n",
        "\n",
        "for (label, article) in tqdm(train_data):\n",
        "    counter.update(tokenizer(article))\n",
        "\n",
        "for (label, article) in tqdm(test_data):\n",
        "    counter.update(tokenizer(article))\n",
        "\n",
        "vocab = vocab(counter, min_freq=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "npFGh5hPHME_"
      },
      "source": [
        "The tokenizer preprocesses an input sentence. More specifically, it converts all characters to lowercase and splits the sentence into a list of words and punctuation marks. For example:\n",
        "\n",
        "    tokenizer(\"You can now install TorchText using pip!\")\n",
        "    >>> ['you', 'can', 'now', 'install', 'torchtext', 'using', 'pip', '!']\n",
        "\n",
        "\n",
        "The vocabulary allows us to convert a list of tokens into integers:\n",
        "\n",
        "    [vocab[token] for token in ['here', 'is', 'an', 'example']]\n",
        "    >>> [879, 179, 92, 2521]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7goiTh42HME_"
      },
      "source": [
        "### Create text and label\n",
        "\n",
        "We employ the tokenizer and vocabulary to process the raw data strings from the dataset iterators. The `text_pipeline` converts a text string into a list of integers based on the lookup table defined in the vocabulary. The label pipeline converts the class label (=genre) into an integer."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SYepTwsbHME_"
      },
      "outputs": [],
      "source": [
        "text_pipeline = lambda x: [vocab[token] for token in tokenizer(x)]\n",
        "label_pipeline = lambda x: int(x) - 1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C1zD3cKtHME_"
      },
      "source": [
        "For example:\n",
        "\n",
        "    text_pipeline('here is the an example')\n",
        "    >>> [475, 21, 2, 30, 5286]\n",
        "    label_pipeline('10')\n",
        "    >>> 9"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hHMk2rjjHME_"
      },
      "source": [
        "### Generate batch iterators\n",
        "\n",
        "We use `torch.utils.data.DataLoader` for creating an input pipeline that processes both the textual input data and labels to make them compatible with a PyTorch model. Additionally, the `DataLoader` provides extra functionalities that are useful in many deep-learning projects (e.g. batching, shuffling and many more). We shall specify how the data points need to be batched using the `collate_fn` method that can be passed as input argument to `DataLoader`. The input of the ``collate_fn`` function is list of $k$ datapoints where $k$ denotes the batch size that was passed in ``DataLoader``. The ``collate_fn`` function then processes each of these $k$ datapoints according to the data processing pipelines declared previously, i.e. by employing the vocabulary, text pipeline and label pipeline.\n",
        "\n",
        "Note that the number of words in each article is not nessarily the same. This often causes difficulties when batching multiple datapoints. To solve this issue, we will zero-pad the sequences, i.e. append zeros such that all sequences have the same length.\n",
        "\n",
        "**Tip:** Ideally, we do not want these zero-padded words to be processed by our rnn since the padded words do not hold any additional information. PyTorch provides functionality for converting input sequences to a memory and computational efficient format by packing all padded sequences, see [stackoverflow](https://stackoverflow.com/questions/51030782/why-do-we-pack-the-sequences-in-pytorch)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nKszpG9ZHMFA"
      },
      "outputs": [],
      "source": [
        "from torch.utils.data import DataLoader\n",
        "from torch.nn.utils.rnn import pad_sequence\n",
        "\n",
        "# check if gpu is available\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "\n",
        "def collate_batch(batch):\n",
        "    \"\"\"\n",
        "    Concatenate multiple datapoints to obtain a single batch of data\n",
        "    \"\"\"\n",
        "\n",
        "    label_list, text_list = [], []\n",
        "\n",
        "    # iterate over each sample in the batch\n",
        "    for (_label, _text) in batch:\n",
        "\n",
        "        # process label using the label_pipeline and\n",
        "        # append this processed label to a list\n",
        "        label_list.append(label_pipeline(_label))\n",
        "\n",
        "        # process the input text using the text_pipeline\n",
        "        # and append this processed text to a list\n",
        "        processed_text = torch.tensor(text_pipeline(_text), dtype=torch.int64)\n",
        "        text_list.append(processed_text)\n",
        "\n",
        "\n",
        "    # convert label_lists to torch.tensor that is compatible with pytorch\n",
        "    label_batch = torch.tensor(label_list, dtype=torch.int64)\n",
        "\n",
        "    # add zeros to ensure that all sequences have same length\n",
        "    padded_text_batch = pad_sequence(sequences = text_list,\n",
        "                                    batch_first=True,\n",
        "                                    padding_value=0.0)\n",
        "\n",
        "    # return labels and articles after transferring them to GPU (if available)\n",
        "    return label_batch.to(device), padded_text_batch.to(device)\n",
        "\n",
        "\n",
        "train_iter = AG_NEWS(split='train')\n",
        "dataloader = DataLoader(train_iter, batch_size=8, collate_fn=collate_batch)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uzJuFs5yHMFB"
      },
      "source": [
        "It is often useful to have access to a single batch of data. For instance, in the model development stage we want to feed a single batch to our model and check if the code runs without errors and bugs. We can extract a single batch from the `DataLoader` in the following way:\n",
        "    \n",
        "    >>> y_batch, x_batch = next(iter(dataloader))\n",
        "    \n",
        "\n",
        "Furthermore, we can retrieve the whole dataset in batches by iterating over the `DataLoader`, i.e.\n",
        "\n",
        "    >>> for batch_idx, (y_batch, x_batch) in enumerate(dataloader):\n",
        "            pass\n",
        "    \n",
        "    \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LZyNYqizHMFB"
      },
      "source": [
        "# Define, train and evaluate the recurrent classification model\n",
        "\n",
        "We employ a long-short-term memory (LSTM) cell that processes each of the words (embeddings) of news articles. The first layer is the Embedding layer that uses a vector with `embed_dim=500` entries representing each word. The next layer is the LSTM layer with 128 neurons representing the hidden state of the LSTM. By default, PyTorch's LSTM initiates hidden state and cell state of zeros and automatically updates the these state after seeing the word embedding of each token in the sequence. If trained properly, the hidden state of the last timestep (i.e. when all tokens in an article have passed) contains all information that is required to predict the article's genre. Finally, via two dense layers (with ReLU nonlinearity inbetween) we map the information of this final hidden state to a vector of length `num_class=4`. The class (i.e. which genre) is obtained by the `torch.argmax` operation. Alternatively, one could assign probabilities to each class through the a softmax activation function after the final dense layer. Similarly, the predicted class is then obtained by checking which class has the highest probability."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "er2Cml3HHMFB"
      },
      "outputs": [],
      "source": [
        "from torch import nn\n",
        "\n",
        "class RNNClassificationModel(nn.Module):\n",
        "\n",
        "    def __init__(self, vocab_size, embed_dim, n_layers, hidden_dim, num_class):\n",
        "        super(RNNClassificationModel, self).__init__()\n",
        "\n",
        "        # save some parameters\n",
        "        self.n_layers = n_layers\n",
        "        self.embed_dim = embed_dim\n",
        "\n",
        "        self.word_embeddings = nn.Embedding(vocab_size, embed_dim)\n",
        "\n",
        "        self.rnn = nn.LSTM(embed_dim, hidden_dim, n_layers, batch_first=True)\n",
        "\n",
        "        self.fc1 = nn.Linear(hidden_dim, hidden_dim)\n",
        "        self.fc2 = nn.Linear(hidden_dim, num_class)\n",
        "\n",
        "        self.drop = nn.Dropout(p=0.5)\n",
        "\n",
        "        self.criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "        self.optimizer = torch.optim.Adam(self.parameters())\n",
        "\n",
        "\n",
        "    def forward(self, text):\n",
        "\n",
        "        # input shapes:\n",
        "        # text: [batch_size, max. sequence length in batch]\n",
        "\n",
        "\n",
        "        # embed sequences with word embedding\n",
        "        # shape [batch_size, max. sequence length in batch, embed_dim]\n",
        "        h = self.word_embeddings(text)\n",
        "\n",
        "        # Feed packed input sequence to lstm\n",
        "        out, (hidden, _) = self.rnn(h)\n",
        "\n",
        "        # retrieve final hidden output of last timestep for each sequence\n",
        "        # shape [batch_size, hidden_dim]\n",
        "        last_timestep = out[:,-1]\n",
        "\n",
        "        # apply dropout\n",
        "        last_timestep = self.drop(last_timestep)\n",
        "\n",
        "        # feed lstm output to MLP, apply ReLU nonlinearity\n",
        "        # shape [batch_size, hidden_dim]\n",
        "        h = self.drop(self.fc1(last_timestep))\n",
        "        h = nn.ReLU()(h)\n",
        "\n",
        "        # shape [batch_size, num_classes]\n",
        "        y_pred = self.fc2(h)\n",
        "\n",
        "        return y_pred"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bRIeK7T8HMFC"
      },
      "outputs": [],
      "source": [
        "import time\n",
        "from torch.nn.functional import softmax\n",
        "\n",
        "def train(dataloader):\n",
        "\n",
        "    model.train()\n",
        "\n",
        "    total_acc, total_count = 0, 0\n",
        "    start_time = time.time()\n",
        "    for (y_true, text_batch) in tqdm(dataloader):\n",
        "\n",
        "        model.optimizer.zero_grad()\n",
        "\n",
        "        y_pred = model(text_batch) #shape (batch_size, num_classes)\n",
        "\n",
        "        y_class = y_pred.argmax(dim=-1)\n",
        "\n",
        "        # alternatively, one can assign probabilities to each class with softmax activation.\n",
        "        # However, torch's CrossEntropyLoss expects the 'logits', which are the model outputs before\n",
        "        # being mapped to probabilities by softmax.\n",
        "\n",
        "#         y_prob = softmax(y_pred, dim=-1)\n",
        "\n",
        "        loss = model.criterion(y_pred, y_true)\n",
        "\n",
        "        loss.backward()\n",
        "\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), 0.1)\n",
        "\n",
        "        model.optimizer.step()\n",
        "\n",
        "\n",
        "\n",
        "        total_acc += (y_class == y_true).sum().item()\n",
        "        total_count += y_true.size(0)\n",
        "\n",
        "\n",
        "\n",
        "    return total_acc/total_count\n",
        "\n",
        "def evaluate(dataloader):\n",
        "    model.eval()\n",
        "    total_acc, total_count = 0, 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for idx, (y_true, text_batch) in enumerate(dataloader):\n",
        "\n",
        "            y_pred = model(text_batch) #shape (batch_size, num_classes)\n",
        "\n",
        "            y_class = y_pred.argmax(dim=-1)\n",
        "\n",
        "#             y_prob = softmax(y_pred, dim=-1)\n",
        "\n",
        "            loss = model.criterion(y_pred, y_true)\n",
        "\n",
        "            total_acc += (y_class == y_true).sum().item()\n",
        "            total_count += y_true.size(0)\n",
        "\n",
        "    return total_acc/total_count"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eNaNXqWNHMFC"
      },
      "source": [
        "### Combine everything, train and evaluate model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s8xbd-zUHMFC"
      },
      "outputs": [],
      "source": [
        "# hyperparameters\n",
        "EPOCHS = 10 # epoch\n",
        "BATCH_SIZE = 256 # batch size for training/validation\n",
        "\n",
        "\n",
        "# initiate recurrent classification model\n",
        "model = RNNClassificationModel(len(vocab),\n",
        "                               embed_dim=500,\n",
        "                               n_layers = 1,\n",
        "                               hidden_dim=128,\n",
        "                               num_class=4).to(device)\n",
        "\n",
        "\n",
        "\n",
        "# training loop\n",
        "train_loss, val_loss = [], []\n",
        "for epoch in range(1, EPOCHS + 1):\n",
        "    epoch_start_time = time.time()\n",
        "\n",
        "    train_dataloader = DataLoader(train_data, batch_size=BATCH_SIZE, collate_fn=collate_batch)\n",
        "    test_dataloader = DataLoader(test_data, batch_size=BATCH_SIZE, collate_fn=collate_batch)\n",
        "\n",
        "    train_loss.append(train(train_dataloader))\n",
        "    val_loss.append(evaluate(test_dataloader))\n",
        "\n",
        "    print('-' * 59)\n",
        "    print('| end of epoch {:3d} | time: {:5.2f}s | '\n",
        "          'valid accuracy {:8.3f} '.format(epoch,\n",
        "                                           time.time() - epoch_start_time,\n",
        "                                           val_loss[-1]))\n",
        "    print('-' * 59)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iHAWpu5AHMFD"
      },
      "outputs": [],
      "source": [
        "f, ax = plt.subplots(1,1, figsize=(10,8))\n",
        "\n",
        "fnt=16\n",
        "ax.plot(train_loss, color='blue', label='Train')\n",
        "ax.plot(val_loss, color='red', linestyle='--', label='Test')\n",
        "ax.legend(fontsize=fnt)\n",
        "ax.tick_params(axis='both', labelsize=fnt)\n",
        "\n",
        "ax.set_xlabel(\"Epoch\", fontsize=fnt)\n",
        "ax.set_ylabel(\"Loss\", fontsize=fnt);\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v2zJemR1HMFD"
      },
      "source": [
        "### Confusion matrix\n",
        "\n",
        "For classification tasks it is often instructive to look at the confusion matrix. This clearly shows which kind of errors are frequent. For instance, we see that class 2 (business) and 3 (sci-tech) are often confused."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Idpg1rfDHMFD"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
        "import seaborn as sns\n",
        "%pylab inline\n",
        "\n",
        "\n",
        "test_iter = AG_NEWS(split='test')\n",
        "test_dataloader = DataLoader(test_iter, batch_size=BATCH_SIZE, collate_fn=collate_batch)\n",
        "\n",
        "\n",
        "# predict in batches the classes of all samples in test dataset\n",
        "y_pred, y_true = [], []\n",
        "for idx, (y, text_batch) in enumerate(test_dataloader):\n",
        "\n",
        "    y_pred.append(model(text_batch).argmax(1))\n",
        "    y_true.append(y)\n",
        "\n",
        "# plot confusion matrix\n",
        "\n",
        "y_true_np = torch.cat(y_true).cpu().numpy()\n",
        "y_pred_np = torch.cat(y_pred).cpu().numpy()\n",
        "\n",
        "cm = confusion_matrix(y_true_np, y_pred_np)\n",
        "f, ax = plt.subplots(1,1,figsize=(12,10))\n",
        "sns.heatmap(cm, annot=True, ax = ax, cmap='Blues', fmt=\"d\", xticklabels=classdict.values(), yticklabels=classdict.values())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sjaYdTbhnWtp"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.8"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}